# Instagram Reels Microservice Environment Configuration
# Copy this file to .env and fill in your actual values

# Service Configuration
SERVICE_HOST=0.0.0.0
SERVICE_PORT=5001
DEBUG=false
MAX_REELS_DEFAULT=10
SCRAPING_TIMEOUT=30
MAX_RETRIES=3

# Mistral AI Configuration (Required)
MISTRAL_API_KEY=your_mistral_api_key_here
MISTRAL_API_URL=https://api.mistral.ai/v1/chat/completions
MISTRAL_MODEL=mistral-tiny
MISTRAL_TEMPERATURE=0.7
MISTRAL_MAX_TOKENS=500

# Instagram Configuration (Optional but recommended)
INSTAGRAM_USERNAME=your_instagram_username
INSTAGRAM_PASSWORD=your_instagram_password
INSTAGRAM_USE_LOGIN=true

# OpenRouter API Configuration
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_API_URL=https://openrouter.ai/api/v1/chat/completions

# Ollama Configuration (for local LLM inference)
OLLAMA_API_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=mistral

# Available models for OpenRouter
# TEXT_MODELS = {
#     "Mistral Small": "mistralai/mistral-small-3.2-24b-instruct:free",
#     "Mistral 3.1": "mistralai/mistral-small-3.1-24b-instruct:free",
#     "Gemma": "google/gemma-3-4b-it:free",
# }

# Database Configuration
DATABASE_URL=sqlite:///llm_analysis.db

# LLM Processing Configuration
LLM_PROVIDER=mistral  # Options: mistral, openrouter, ollama
LLM_MAX_TOKENS=1000
LLM_TEMPERATURE=0.7
LLM_TIMEOUT=30

# Optional: Additional configuration
# LOG_LEVEL=INFO
# CORS_ORIGINS=*
# RATE_LIMIT_PER_MINUTE=60